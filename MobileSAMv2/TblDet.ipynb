{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eliaweiss/ai/segmentation/MobileSAM\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from TableDetect import TableDetect\n",
    "from MobileSamBoxes import MobileSamBoxes\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "  \n",
    "def plot_results(model, pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax().item()\n",
    "        score = p[cl]\n",
    "        text = f'{model.config.id2label[cl]}: {score:0.2f}'\n",
    "        \n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "        \n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    img = np.ones((anns.shape[1], anns.shape[2], 4))\n",
    "    # img[:,:,3] = 0\n",
    "    for ann in range(anns.shape[0]):\n",
    "        m = anns[ann].bool()\n",
    "        m=m.cpu().numpy()\n",
    "        color_mask = np.concatenate([np.random.random(3), [1]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "def plot_annotations(anns):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    background=np.ones_like(sam.image)*255\n",
    "    plt.imshow(background)\n",
    "    show_anns(anns)\n",
    "    plt.axis('off')\n",
    "    plt.show() \n",
    "    # plt.savefig(\"{}\".format(\"./out/result.jpg\"), bbox_inches='tight', pad_inches = 0.0)         \n",
    "\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def outputs_to_objects(outputs, img_size, id2label):\n",
    "    m = outputs.logits.softmax(-1).max(-1)\n",
    "    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
    "    pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
    "    pred_bboxes = outputs['pred_boxes'].detach().cpu()[0]\n",
    "    pred_bboxes = [elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)]\n",
    "\n",
    "    objects = []\n",
    "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
    "        class_label = id2label[int(label)]\n",
    "        if not class_label == 'no object':\n",
    "            objects.append({'label': class_label, 'score': float(score),\n",
    "                            'bbox': [float(elem) for elem in bbox]})\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"./app/assets/picture1.jpg\"\n",
    "# file_path = \"/Users/eliaweiss/Documents/doc2txt/sihach/attachments/img/20231025012642_001.jpg\"\n",
    "file_path = \"/Users/eliaweiss/Documents/doc2txt/sihach/need fix/4e7d4235deff4ee2b72d886e12ad0bcf.jpg\"\n",
    "\n",
    "# file_path = \"/Users/eliaweiss/Documents/doc2txt/invoices 2/expense (6).jpeg\"\n",
    "\n",
    "img = Image.open(file_path)\n",
    "tblDec = TableDetect()\n",
    "probas, boxes = tblDec.detectTables(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(tblDec.model, img, probas, boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop table with sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = MobileSamBoxes(img)\n",
    "anns = sam.process(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_annotations(anns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def create_mask_patch(image, mask):\n",
    "  \"\"\"\n",
    "  Creates a patch from the image containing only the mask region.\n",
    "\n",
    "  Args:\n",
    "      image: A PIL image object.\n",
    "      mask: A NumPy array representing the mask (1 for mask pixels).\n",
    "\n",
    "  Returns:\n",
    "      A PIL image object containing the patch from the original image \n",
    "      with only the mask region and white background for non-mask pixels.\n",
    "  \"\"\"\n",
    "  mask=mask.cpu().numpy()\n",
    "  # mask = cv2.dilate(mask, np.ones((13, 13), np.uint8), iterations=1)  \n",
    "  # Find non-zero elements (mask pixels) in the mask\n",
    "  mask_coords = np.where(mask != 0)\n",
    "\n",
    "  # Get minimum and maximum coordinates from the mask pixels\n",
    "  min_y, min_x = mask_coords[0].min(), mask_coords[1].min()\n",
    "  max_y, max_x = mask_coords[0].max() + 1, mask_coords[1].max() + 1  # Add 1 for inclusive range\n",
    "\n",
    "  # Create the bounding box rectangle\n",
    "  bounding_box = (min_x, min_y, max_x, max_y)\n",
    "\n",
    "\n",
    "  mask = mask==1\n",
    "  imgArr = np.array(image)\n",
    "  imgArr[~mask] = (255,255,255)\n",
    "  image = Image.fromarray(imgArr)\n",
    "  # color_mask = np.concatenate([np.random.random(3), [1]])\n",
    "  # img[m] = color_mask\n",
    "  # Crop the image using the bounding box\n",
    "  patch_image = image.crop(bounding_box)\n",
    "  return patch_image\n",
    "\n",
    "# Example usage\n",
    "mask = anns[0]\n",
    "\n",
    "tbl_patch = create_mask_patch(img, mask)\n",
    "# patch.show()  # Display the created patch\n",
    "tbl_patch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table structure recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrFeatureExtractor\n",
    "\n",
    "feature_extractor = DetrFeatureExtractor()\n",
    "encoding = feature_extractor(tbl_patch, return_tensors=\"pt\")\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TableTransformerForObjectDetection\n",
    "\n",
    "# new v1.1 checkpoints require no timm anymore\n",
    "structure_model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-structure-recognition-v1.1-all\")\n",
    "structure_model.to(device)\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class MaxResize(object):\n",
    "    def __init__(self, max_size=800):\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        width, height = image.size\n",
    "        current_max_size = max(width, height)\n",
    "        scale = self.max_size / current_max_size\n",
    "        resized_image = image.resize((int(round(scale*width)), int(round(scale*height))))\n",
    "\n",
    "        return resized_image\n",
    "structure_transform = transforms.Compose([\n",
    "    MaxResize(1000),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = structure_transform(tbl_patch).unsqueeze(0)\n",
    "pixel_values = pixel_values.to(device)\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = structure_model(pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update id2label to include \"no object\"\n",
    "structure_id2label = structure_model.config.id2label\n",
    "structure_id2label[len(structure_id2label)] = \"no object\"\n",
    "\n",
    "cells = outputs_to_objects(outputs, tbl_patch.size, structure_id2label)\n",
    "print(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "cropped_table_visualized = tbl_patch.copy()\n",
    "draw = ImageDraw.Draw(cropped_table_visualized)\n",
    "\n",
    "for cell in cells:\n",
    "    draw.rectangle(cell[\"bbox\"], outline=\"red\")\n",
    "\n",
    "cropped_table_visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(tbl_patch, cells, class_to_visualize):\n",
    "    if class_to_visualize not in structure_model.config.id2label.values():\n",
    "      raise ValueError(f\"Class should be one of the available classes {structure_model.config.id2label}\")\n",
    "\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(tbl_patch)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    for cell in cells:\n",
    "        bbox = cell[\"bbox\"]\n",
    "        label = cell[\"label\"]\n",
    "\n",
    "        if label == class_to_visualize:\n",
    "          xmin, ymin, xmax, ymax = tuple(bbox)\n",
    "\n",
    "          ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=np.random.random(3), linewidth=3))\n",
    "          plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_cells(tbl_patch,cells, class_to_visualize=\"table row\")\n",
    "plot_results_cells(tbl_patch, cells, class_to_visualize=\"table column\")\n",
    "plot_results_cells(tbl_patch,cells, class_to_visualize=\"table column header\")\n",
    "# plot_results_cells(tbl_patch,cells, class_to_visualize=\"table projected row header\")\n",
    "# plot_results_cells(tbl_patch,cells, class_to_visualize=\"table spanning cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# align table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = anns[0].bool()\n",
    "m=m.cpu().numpy()\n",
    "mask = np.zeros((anns.shape[1], anns.shape[2], 1), np.uint8)\n",
    "mask[m] = 255\n",
    "# mask = ~mask\n",
    "print(mask.shape)\n",
    "print(mask.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contour = contours[0]\n",
    "len(contours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcBB():\n",
    "    mask_coords = np.where(mask != 0)\n",
    "\n",
    "# Get minimum and maximum coordinates from the mask pixels\n",
    "    min_y, min_x = mask_coords[0].min(), mask_coords[1].min()\n",
    "    max_y, max_x = mask_coords[0].max() + 1, mask_coords[1].max() + 1  # Add 1 for inclusive range\n",
    "\n",
    "# Define the dimensions of the output rectangle\n",
    "    width = max_x-min_x  # Define your desired width here\n",
    "    height = max_y-min_y  # Define your desired height here\n",
    "    return (min_x, max_x), (min_y, max_y), (width,height)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation_midpoint(p1,p2):\n",
    "  (x1, y1) = p1\n",
    "  (x2, y2) = p2\n",
    "  # Calculate the slopes in x and y directions\n",
    "  slope_x = (x2 - x1) / 2\n",
    "  slope_y = (y2 - y1) / 2\n",
    "\n",
    "  # Calculate the midpoint coordinates\n",
    "  midpoint_x = x1 + slope_x\n",
    "  midpoint_y = y1 + slope_y\n",
    "\n",
    "  return midpoint_x, midpoint_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def find_closest_point_in_contour(contour, point_xy):\n",
    "   \"\"\"\n",
    "   Finds the closest point within a contour to a given point.\n",
    "\n",
    "   Args:\n",
    "       contour: A NumPy array representing the contour, where each row is a (x, y) point.\n",
    "       point_xy: A tuple of (x, y) coordinates for the reference point.\n",
    "\n",
    "   Returns:\n",
    "       A tuple representing the (x, y) coordinates of the closest point in the contour.\n",
    "   \"\"\"\n",
    "\n",
    "   min_dist = np.inf\n",
    "   closest_point = None\n",
    "\n",
    "   for contour_point in contour:\n",
    "       x, y = contour_point[0]\n",
    "       dist = np.sqrt((x - point_xy[0])**2 + (y - point_xy[1])**2)\n",
    "       if dist < min_dist:\n",
    "           min_dist = dist\n",
    "           closest_point = x, y\n",
    "\n",
    "#    return closest_point\n",
    "   return linear_interpolation_midpoint(closest_point,point_xy)\n",
    "\n",
    "def approxBoundingQuadrilateral(find_closest_point_in_contour):\n",
    "    (min_x, max_x), (min_y, max_y), (width,height) = calcBB()\n",
    "    lb = find_closest_point_in_contour(contour, (min_x,min_y))\n",
    "    lt = find_closest_point_in_contour(contour, (min_x,max_y))\n",
    "    rt = find_closest_point_in_contour(contour, (max_x,max_y))\n",
    "    rb = find_closest_point_in_contour(contour, (max_x,min_y))\n",
    "    # return clock wise\n",
    "    return (lb,rb, rt, lt), (width,height)\n",
    "\n",
    "boundingQuadrilateral, (width,height) = approxBoundingQuadrilateral(find_closest_point_in_contour)\n",
    "boundingQuadrilateral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the vertices of the quadrilateral to a numpy array\n",
    "# pts1 = np.float32(approx)\n",
    "pts1 = np.float32(boundingQuadrilateral)\n",
    "\n",
    "# Define the corners of the rectangle\n",
    "# pts2 = np.float32([[0, 0], [0, height], [width, height] , [width, 0]])\n",
    "pts2 = np.float32([[0, 0], [width, 0], [width, height], [0, height]]) \n",
    "\n",
    "\n",
    "# Compute the perspective transformation matrix\n",
    "matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    " \n",
    "# Apply the perspective transformation to the original image\n",
    "result = cv2.warpPerspective(image, matrix, (width, height))\n",
    "\n",
    "# Display the result\n",
    "# cv2.imshow('Aligned Image', result)\n",
    "# cv2.waitKey(0)\n",
    "alignTable = Image.fromarray(result)\n",
    "alignTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find cell from align tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = structure_transform(alignTable).unsqueeze(0)\n",
    "pixel_values = pixel_values.to(device)\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = structure_model(pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update id2label to include \"no object\"\n",
    "structure_id2label = structure_model.config.id2label\n",
    "structure_id2label[len(structure_id2label)] = \"no object\"\n",
    "\n",
    "cells = outputs_to_objects(outputs, alignTable.size, structure_id2label)\n",
    "print(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_cells(alignTable,cells, class_to_visualize=\"table row\")\n",
    "plot_results_cells(alignTable, cells, class_to_visualize=\"table column\")\n",
    "plot_results_cells(alignTable,cells, class_to_visualize=\"table column header\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the inverse homography matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the inverse homography matrix\n",
    "\n",
    "pts3 = pts1.copy()\n",
    "(min_x, max_x), (min_y, max_y), (width,height) = calcBB()\n",
    "pts3[:,0] -= min_x\n",
    "pts3[:,1] -= min_y\n",
    "inv_matrix = cv2.findHomography(pts2, pts3)[0]\n",
    "\n",
    "# Reverse warp the image\n",
    "reversed_result = cv2.warpPerspective(result, inv_matrix, (width, height))\n",
    "reversedTbl = Image.fromarray(reversed_result)\n",
    "reversedTbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobileSam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
